build-lists: true

# [fit] Bad People, 

# [fit] Bad Computers

by _@tcburning_

---

![](img/me.jpg)

---

![](img/jim.gif)

---

#THROWBACK: CS 101

---

```python
number = random.randint(1, 10)
guess = raw_input('Guess a number 1-10. If you are right, you win a million bucks: ')
if (guess == number):
  print("Omg, you win a million bucks! Ask someone in the audience for it.")
else
  print("Awkward...")
```

---

```python
Guess a number 1-10. If you are right, you win a million bucks: 20
```

---

```python
if (guess > 10):
  print("Make sure your guess is between 1 and 10, inclusive")
elif (guess == number):
  print("Omg, you win a million bucks! Ask someone in the audience for it.")
else
  print("Awkward...")
```

---

```python
Guess a number 1-10. If you are right, you win a million bucks: two
```

---

```python
#function thats converts strings to integers
def convert(wordString):
  if(wordString == 'one'):
    return 1
  elif(wordString == 'two'):
    return 2
  [...and so on...]

#convert if the user happens to enter a string
if(isinstance(guess, str)):
  guess = convert(guess)
```

---

```python
Guess a number 1-10. If you are right, you win a million bucks: 

- No comprendo

- Je ne comprends pas

- Ek verstaan nie

[...and so on...]

```

---

What if:

- Someone can't see the screen?

- Someone can only count to five?

- Someone doesn't know how to interact with a computer?

---

![](img/scary.gif)

---

![](img/taygood.png)

---

![inline fill](img/taybad1.png)![inline fill](img/taybad2.png)
![inline](img/taybad3.png)

---

![fit](img/googlegood.png)

---

![inline fill](img/googlebad1.png)![inline](img/googlebad2.png)
![inline](img/googlebad3.png)

---

![inline](img/women1.png)![inline fill](img/women2.png)
![inline](img/women3.png)

---

![inline](img/vr1.png)![inline fill](img/vr2.png)
![inline](img/vr3.png)

---

![](img/sleepybaby.gif)

---

###Is an algorithm any less racist than a human?

#### "But there’s an unaddressed issue here: any algorithm can – and often does – simply reproduce the biases inherent in its creator, in the data it’s using, or in society at large. For example, Google is more likely to advertise executive-level salaried positions to search engine users if it thinks the user is male, according to a Carnegie Mellon study. While Harvard researchers found that ads about arrest records were much more likely to appear alongside searches for names thought to belong to a black person versus a white person...

---

### "These aren’t necessarily malicious situations – it’s not that Google is staffed by sexists, for example, but rather that the algorithm is just mirroring the existing gender pay gap. But in so doing, the algorithm reinforces that gap, and as long as we continue to believe an algorithm is an “unbiased” machine, we risk reinforcing the status quo in harmful ways. 

--- The Guardian

---

![left](img/icecube.gif)

> You better check yo self before you wreck yo self

--- Ice Cube: Highly respected prophet and software engineer

---

###Be Cognizant of Implicit Bias...

aka, check your privilege

---

###... And how that Manifests in Code

- Please pay $5 to play this game.

- Please enter the college you attended to confirm your identity.

- Please enter your direct manager's contact info to verify your work history.

---

###Be Cognizant of Programming Bias

#### AngularJS vs React

#### Cassandra vs MongoDB

#### Server-side vs client-side

#### Java vs C++

and so on

---

###Diverse teams, companies, thoughts, backgrounds, music tastes, etc etc

![](img/emoji.jpg)

---

###The Future of Computing - Building at Unprecedented Scale:

- Machine learning

- Big Data

- Web Services

- Security Infrastructure

- ...and more!

---

###For the first time in history we are learning what it means to design and engineer products for everyone the world.

---

![](img/scary.gif)

---

- Bad People

- Bad Computers

---

> “You can’t ever reach perfection, but you can believe in an asymptote toward which you are ceaselessly striving.” 

---Paul Kalanithi, When Breath Becomes Air

---

#Thank you.

---

Sources

[Why Carmakers Always Insisted on Male Crash-Test Dummies](https://www.bloomberg.com/view/articles/2012-08-22/why-carmakers-always-insisted-on-male-crash-test-dummies)

[Is an algorithm any less racist than a human?](https://www.theguardian.com/technology/2016/aug/03/algorithm-racist-human-employers-work)

Screenshots of articles reference their source.
